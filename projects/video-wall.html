<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="next-head-count" content="2"/><link rel="preload" href="/_next/static/css/7ca27de87800404a.css" as="style"/><link rel="stylesheet" href="/_next/static/css/7ca27de87800404a.css" data-n-g=""/><link rel="preload" href="/_next/static/css/d74e702482a9151e.css" as="style"/><link rel="stylesheet" href="/_next/static/css/d74e702482a9151e.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-9b312e20a4e32339.js" defer=""></script><script src="/_next/static/chunks/framework-4556c45dd113b893.js" defer=""></script><script src="/_next/static/chunks/main-760b458b5ffc5c05.js" defer=""></script><script src="/_next/static/chunks/pages/_app-b532ff70e77d9beb.js" defer=""></script><script src="/_next/static/chunks/454-43338d3b48f37cdf.js" defer=""></script><script src="/_next/static/chunks/pages/projects/%5Bid%5D-120d18fc22ab4300.js" defer=""></script><script src="/_next/static/4uJRkLhUcV2vGKFOe8tYn/_buildManifest.js" defer=""></script><script src="/_next/static/4uJRkLhUcV2vGKFOe8tYn/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="App_mainContainer__2hJsN"><main class="App_mainColumn__6EkS0"><header class="App_header__kD6M4"><nav class="App_nav__c9ITJ"><div><a href="/">cannoneyed</a></div>·<div><a href="/about">about</a></div>·<div><a href="/projects">projects</a></div></nav></header><div><h3 class="Project_title__D23TS">The M Machine - 3D virtual light wall</h3><div class="Project_paragraph__8kxkB"><div class="Project_figure__0gdBT"><img class="Project_image__orJC9" src="/img/projects/thumbnail-video-wall.jpg" alt="" loading="lazy"/><figcaption class="Project_caption__Pt3Zi"></figcaption></div></div>
<div class="Project_paragraph__8kxkB">In early 2013 The M Machine were invited to play at Ultra Music Festival in Miami, and we began the process of designing a new live show complete with an entirely new visual package. While the physical M light wall was a truly impressive stagepiece, it was impractical to tour with, particularly for festival shows where stage transition time is very limited.</div>
<div class="Project_figure__0gdBT"><video class="Project_video__N_3GV" src="/img/projects/m-video-wall-03.webm" width="100%" autoplay="" loop="" muted=""></video><figcaption class="Project_caption__Pt3Zi">The virtual light wall in action on massive LED walls at Lucky Seattle, 2013</figcaption></div>
<div class="Project_paragraph__8kxkB">We elected to build a completely digital version of our physical light wall, which could do everything the LED version could (responding to the OSC output of our original synchronized visual package) with the added trick of being able to be fully composited with digital video.</div>
<div class="Project_paragraph__8kxkB">Using an amazing program called <a href="https://www.derivative.ca/">TouchDesigner</a>, I was able to composite a real-time generated 3D model of our M Light wall with pre-rendered 3D background video content by our frequent collaborator and brilliant motion graphics maven <a href="http://www.neither-field.com/">Scott Pagano</a>. Since TouchDesigner can ingest almost any kind of data, including OSC, we were able to send the same signals that drove the LED sequences of the physical light wall from Max into TouchDesigner, wire them to lights, and create a fully digital working version of our M.</div>
<div class="Project_paragraph__8kxkB"><div class="Project_figure__0gdBT"><img class="Project_image__orJC9" src="/img/projects/video-wall-collage-1.jpg" alt="Many late nights spent hacking Touch Designer with Phil Reyneri" loading="lazy"/><figcaption class="Project_caption__Pt3Zi">Many late nights spent hacking Touch Designer with Phil Reyneri</figcaption></div></div>
<div class="Project_paragraph__8kxkB">But perhaps the coolest feature of the virtual M was it&#x27;s ability to <strong>play</strong> video on the panels themselves. By capturing video off of the GPU using <a href="http://syphon.v002.info/recorder/">syphon recorder</a>, we were able to take the output of my <a href="https://resolume.com/">Resolume</a> VJ software and composite it into our TouchDesigner rig to display it in in real time on the face of the M. The live show thus became centered around an idea: The M would display different video content corresponding to each song in the performance. The visual show would consist of zooming into the M until you were immersed in the content, then zooming back out to show the visuals being displayed on the face of the M.</div>
<div class="Project_figure__0gdBT"><video class="Project_video__N_3GV" src="/img/projects/m-video-wall-05.webm" width="100%" autoplay="" loop="" muted=""></video><figcaption class="Project_caption__Pt3Zi">Sequenced video for Shinichi Osawa&#x27;s remix of Tiny Anthem</figcaption></div>
<div class="Project_paragraph__8kxkB">Of course, it wasn&#x27;t enough to be able to just play video on the virtual M, I had to sequence it and perform with it as well. So I built another OSC-based system for Ableton to sequence video clips and effects and resolume, as well as transitioning between scenes in Touch Designer. When all of it was working together, it was a massive scale orchestration of three machines and a multitude of controllers and sequencers generating a highly synchronized and deeply detailed visual performance.</div>
<div class="Project_figure__0gdBT"><video class="Project_video__N_3GV" src="/img/projects/m-video-wall-04.webm" width="100%" autoplay="" loop="" muted=""></video><figcaption class="Project_caption__Pt3Zi">Austin, TX - 2013</figcaption></div>
<div class="Project_paragraph__8kxkB">One of the most satisfying pieces of switching from the LED to the virtual light wall was the incredible array creative frontiers it opened up. Programming visuals for the LED light wall was truly a unique art form, and I felt as if I had perfected it around my 30th composition - there&#x27;s only so many ways to make 32 pixels and 20 channels of LED ribbon look compelling. However the world of video and VFX is almost limitless, and I sampled thousands of videos of countless styles, mixing, mashing, and effecting them in countless ways to create new and interesting visual content for every track that I programmed accompanying visuals. From <strong>Black</strong>, which featured a strobing black, white, and red 3D spider, to <strong>Luma</strong>, which featured beautiful space imagery, the live show never felt stagnant, repetitive, or forced. And I could always invent new interpretations of the original sequenced data - <strong>Deep Search</strong>, for example, featured a minimalistic 3D pure geometric version of the original LED wall sequence that could be bent and distorted in real time with my MIDI controller.</div>
<div class="Project_paragraph__8kxkB"><div class="Project_figure__0gdBT"><img class="Project_image__orJC9" src="/img/projects/video-wall-collage-2.jpg" alt="Ableton, MAX/MSP, Touch Designer, and Resolume" loading="lazy"/><figcaption class="Project_caption__Pt3Zi">Ableton, MAX/MSP, Touch Designer, and Resolume</figcaption></div></div>
<div class="Project_figure__0gdBT"><video class="Project_video__N_3GV" src="/img/projects/m-video-wall-01.webm" width="100%" autoplay="" loop="" muted=""></video><figcaption class="Project_caption__Pt3Zi"></figcaption></div>
<div class="Project_figure__0gdBT"><video class="Project_video__N_3GV" src="/img/projects/m-video-wall-02.webm" width="100%" autoplay="" loop="" muted=""></video><figcaption class="Project_caption__Pt3Zi"></figcaption></div></div><footer><section><p></p></section></footer></main></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"projectMetadata":{"title":"The M Machine - 3D virtual light wall","date":"03/01/2013","description":"3D video composite concert visuals","thumbnail":"/img/projects/thumbnail-video-wall.jpg","id":"video-wall"},"postContent":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    img: \"img\",\n    a: \"a\",\n    strong: \"strong\"\n  }, _provideComponents(), props.components), {Video} = _components;\n  if (!Video) _missingMdxReference(\"Video\", true);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/img/projects/thumbnail-video-wall.jpg\",\n        alt: \"\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"In early 2013 The M Machine were invited to play at Ultra Music Festival in Miami, and we began the process of designing a new live show complete with an entirely new visual package. While the physical M light wall was a truly impressive stagepiece, it was impractical to tour with, particularly for festival shows where stage transition time is very limited.\"\n    }), \"\\n\", _jsx(Video, {\n      src: \"/img/projects/m-video-wall-03.webm\",\n      caption: \"The virtual light wall in action on massive LED walls at Lucky Seattle, 2013\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"We elected to build a completely digital version of our physical light wall, which could do everything the LED version could (responding to the OSC output of our original synchronized visual package) with the added trick of being able to be fully composited with digital video.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Using an amazing program called \", _jsx(_components.a, {\n        href: \"https://www.derivative.ca/\",\n        children: \"TouchDesigner\"\n      }), \", I was able to composite a real-time generated 3D model of our M Light wall with pre-rendered 3D background video content by our frequent collaborator and brilliant motion graphics maven \", _jsx(_components.a, {\n        href: \"http://www.neither-field.com/\",\n        children: \"Scott Pagano\"\n      }), \". Since TouchDesigner can ingest almost any kind of data, including OSC, we were able to send the same signals that drove the LED sequences of the physical light wall from Max into TouchDesigner, wire them to lights, and create a fully digital working version of our M.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/img/projects/video-wall-collage-1.jpg\",\n        alt: \"Many late nights spent hacking Touch Designer with Phil Reyneri\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"But perhaps the coolest feature of the virtual M was it's ability to \", _jsx(_components.strong, {\n        children: \"play\"\n      }), \" video on the panels themselves. By capturing video off of the GPU using \", _jsx(_components.a, {\n        href: \"http://syphon.v002.info/recorder/\",\n        children: \"syphon recorder\"\n      }), \", we were able to take the output of my \", _jsx(_components.a, {\n        href: \"https://resolume.com/\",\n        children: \"Resolume\"\n      }), \" VJ software and composite it into our TouchDesigner rig to display it in in real time on the face of the M. The live show thus became centered around an idea: The M would display different video content corresponding to each song in the performance. The visual show would consist of zooming into the M until you were immersed in the content, then zooming back out to show the visuals being displayed on the face of the M.\"]\n    }), \"\\n\", _jsx(Video, {\n      src: \"/img/projects/m-video-wall-05.webm\",\n      caption: \"Sequenced video for Shinichi Osawa's remix of Tiny Anthem\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Of course, it wasn't enough to be able to just play video on the virtual M, I had to sequence it and perform with it as well. So I built another OSC-based system for Ableton to sequence video clips and effects and resolume, as well as transitioning between scenes in Touch Designer. When all of it was working together, it was a massive scale orchestration of three machines and a multitude of controllers and sequencers generating a highly synchronized and deeply detailed visual performance.\"\n    }), \"\\n\", _jsx(Video, {\n      src: \"/img/projects/m-video-wall-04.webm\",\n      caption: \"Austin, TX - 2013\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"One of the most satisfying pieces of switching from the LED to the virtual light wall was the incredible array creative frontiers it opened up. Programming visuals for the LED light wall was truly a unique art form, and I felt as if I had perfected it around my 30th composition - there's only so many ways to make 32 pixels and 20 channels of LED ribbon look compelling. However the world of video and VFX is almost limitless, and I sampled thousands of videos of countless styles, mixing, mashing, and effecting them in countless ways to create new and interesting visual content for every track that I programmed accompanying visuals. From \", _jsx(_components.strong, {\n        children: \"Black\"\n      }), \", which featured a strobing black, white, and red 3D spider, to \", _jsx(_components.strong, {\n        children: \"Luma\"\n      }), \", which featured beautiful space imagery, the live show never felt stagnant, repetitive, or forced. And I could always invent new interpretations of the original sequenced data - \", _jsx(_components.strong, {\n        children: \"Deep Search\"\n      }), \", for example, featured a minimalistic 3D pure geometric version of the original LED wall sequence that could be bent and distorted in real time with my MIDI controller.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/img/projects/video-wall-collage-2.jpg\",\n        alt: \"Ableton, MAX/MSP, Touch Designer, and Resolume\"\n      })\n    }), \"\\n\", _jsx(Video, {\n      src: \"/img/projects/m-video-wall-01.webm\"\n    }), \"\\n\", _jsx(Video, {\n      src: \"/img/projects/m-video-wall-02.webm\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","frontmatter":{},"scope":{}},"id":"video-wall"},"__N_SSG":true},"page":"/projects/[id]","query":{"id":"video-wall"},"buildId":"4uJRkLhUcV2vGKFOe8tYn","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>